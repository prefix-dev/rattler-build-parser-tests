[
  {
    "build_configuration": {
      "build_platform": {
        "platform": "linux-64",
        "virtual_packages": [
          "__unix=0=0",
          "__linux=6.6.87.2=0",
          "__glibc=2.35=0",
          "__cuda=13.1=0",
          "__archspec=1=x86_64_v4"
        ]
      },
      "channel_priority": "strict",
      "channels": [
        "https://conda.anaconda.org/conda-forge"
      ],
      "directories": {
        "build_dir": "/home/wolfv/Programs/rattler-build/output/bld/rattler-build_llama.cpp",
        "build_prefix": "/home/wolfv/Programs/rattler-build/output/bld/rattler-build_llama.cpp/build_env",
        "host_prefix": "/home/wolfv/Programs/rattler-build/output/bld/rattler-build_llama.cpp/host_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeh",
        "work_dir": "/home/wolfv/Programs/rattler-build/output/bld/rattler-build_llama.cpp/work"
      },
      "hash": {
        "hash": "d5c5c38"
      },
      "host_platform": {
        "platform": "win-64",
        "virtual_packages": [
          "__win=0=0",
          "__cuda=13.1=0",
          "__archspec=1=x86_64"
        ]
      },
      "packaging_settings": {
        "archive_type": "conda",
        "compression_level": 15
      },
      "solve_strategy": "highest",
      "subpackages": {
        "llama.cpp": {
          "build_string": "cuda129_hd5c5c38_0",
          "name": "llama.cpp",
          "version": "7700"
        }
      },
      "target_platform": "win-64",
      "timestamp": "2026-01-22T16:46:23.619106319Z",
      "variant": {
        "__cuda": "__cuda",
        "blas_impl": "mkl",
        "build_platform": "linux-64",
        "c_compiler": "vs2022",
        "c_stdlib": "vs",
        "channel_sources": "conda-forge",
        "channel_targets": "conda-forge main",
        "cuda_compiler": "cuda-nvcc",
        "cuda_compiler_version": "12.9",
        "cxx_compiler": "vs2022",
        "libcurl": "8",
        "mkl": "2025",
        "target_platform": "win-64"
      }
    },
    "extra_meta": {},
    "finalized_dependencies": null,
    "finalized_sources": null,
    "recipe": {
      "about": {
        "homepage": "https://github.com/ggml-org/llama.cpp",
        "license": "MIT",
        "license_file": [
          "LICENSE"
        ],
        "repository": "https://github.com/ggml-org/llama.cpp",
        "summary": "Port of Facebook's LLaMA model in C/C++"
      },
      "build": {
        "dynamic_linking": {
          "missing_dso_allowlist": [
            "*/nvcuda.dll"
          ]
        },
        "number": 200,
        "script": "echo hello\nset LLAMA_ARGS=-DLLAMA_BUILD_TESTS=OFF\n\n{% macro llama_args(value) -%}\nset LLAMA_ARGS=%LLAMA_ARGS% -DLLAMA_${{ value }}\n{%- endmacro %}\n\n{% macro cmake_args(value) -%}\nset LLAMA_ARGS=%LLAMA_ARGS% -D${{ value }}\n{%- endmacro %}\n\n{% macro ggml_args(value) -%}\nset LLAMA_ARGS=%LLAMA_ARGS% -DGGML_${{ value }}\n{%- endmacro %}\n\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n${{ llama_args(\"CURL=ON\") }}\n\n${{ ggml_args(\"NATIVE=OFF\") }}\n\n{%- if cuda_compiler_version | default(\"None\") != \"None\" %}\n${{ ggml_args(\"CUDA=ON\") }}\n:: Exclude sm_120 (compute capability 12.0) as it doesn't support MXFP4 instructions\n:: Explicitly list all architectures except 120\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=80-virtual;86-real;89-real;120a-real;121a-real\") }}\n\n{%- else %}\n\n${{ ggml_args(\"BLAS=ON\") }}\n\n{%- if blas_impl == \"mkl\" %}\n${{ ggml_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n\n{%- endif %}\n\nset LLAMA_ARGS\ncmake -S . -B build -G Ninja %CMAKE_ARGS% %LLAMA_ARGS%\ncmake --build build\ncmake --install build\n",
        "string": "cuda129_hd5c5c38_0"
      },
      "context": {
        "build": 0,
        "name": "llama.cpp",
        "version": "7700"
      },
      "extra": {
        "recipe-maintainers": [
          "traversaro",
          "jjerphan",
          "jonashaag",
          "frankier",
          "sodre",
          "pavelzw"
        ]
      },
      "package": {
        "name": "llama.cpp",
        "version": "7700"
      },
      "requirements": {
        "build": [
          "vs2022_win-64",
          "vs_win-64",
          "vs2022_win-64",
          "cuda-nvcc_win-64 12.9.*",
          "cmake",
          "git",
          "ninja",
          "pkgconfig"
        ],
        "host": [
          "libcurl",
          "cuda-version 12.9.*",
          "cuda-cudart-dev 12.9.*",
          "libcublas-dev 12.9.*"
        ],
        "run": [
          "cuda-version 12.9.*",
          "__cuda",
          "cuda-nvcc-tools"
        ],
        "run_constraints": [
          "whisper.cpp <0.0.0a0"
        ]
      },
      "schema_version": 1,
      "source": [
        {
          "sha256": "3cfb6fbe53c6d35a4ed87870da08df2ed545c77df10e4a1a193d82cfa2b8c359",
          "url": "https://github.com/ggml-org/llama.cpp/archive/b7700.tar.gz"
        }
      ]
    },
    "system_tools": {
      "rattler-build": "0.55.1"
    }
  }
]