[
  {
    "build_configuration": {
      "build_platform": {
        "platform": "linux-64",
        "virtual_packages": [
          "__unix=0=0",
          "__linux=6.6.87.2=0",
          "__glibc=2.35=0",
          "__cuda=13.1=0",
          "__archspec=1=x86_64_v4"
        ]
      },
      "channel_priority": "strict",
      "channels": [
        "https://conda.anaconda.org/conda-forge"
      ],
      "directories": {
        "build_dir": "/home/wolfv/Programs/rattler-build/output/bld/rattler-build_llama.cpp",
        "build_prefix": "/home/wolfv/Programs/rattler-build/output/bld/rattler-build_llama.cpp/build_env",
        "host_prefix": "/home/wolfv/Programs/rattler-build/output/bld/rattler-build_llama.cpp/host_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placeh",
        "work_dir": "/home/wolfv/Programs/rattler-build/output/bld/rattler-build_llama.cpp/work"
      },
      "hash": {
        "hash": "0393721"
      },
      "host_platform": {
        "platform": "linux-64",
        "virtual_packages": [
          "__unix=0=0",
          "__linux=6.6.87.2=0",
          "__glibc=2.35=0",
          "__cuda=13.1=0",
          "__archspec=1=x86_64_v4"
        ]
      },
      "packaging_settings": {
        "archive_type": "conda",
        "compression_level": 15
      },
      "solve_strategy": "highest",
      "subpackages": {
        "llama.cpp": {
          "build_string": "cpu_mkl_h0393721_0",
          "name": "llama.cpp",
          "version": "7700"
        }
      },
      "target_platform": "linux-64",
      "timestamp": "2026-01-23T07:41:12.442842303Z",
      "variant": {
        "blas_impl": "mkl",
        "build_platform": "linux-64",
        "c_compiler": "gcc",
        "c_compiler_version": "14",
        "c_stdlib": "sysroot",
        "c_stdlib_version": "2.17",
        "channel_sources": "conda-forge",
        "channel_targets": "conda-forge main",
        "cuda_compiler_version": "None",
        "cxx_compiler": "gxx",
        "cxx_compiler_version": "14",
        "libcurl": "8",
        "mkl": "2025",
        "target_platform": "linux-64"
      }
    },
    "extra_meta": {},
    "finalized_dependencies": null,
    "finalized_sources": null,
    "recipe": {
      "about": {
        "homepage": "https://github.com/ggml-org/llama.cpp",
        "license": "MIT",
        "license_file": [
          "LICENSE"
        ],
        "repository": "https://github.com/ggml-org/llama.cpp",
        "summary": "Port of Facebook's LLaMA model in C/C++"
      },
      "build": {
        "dynamic_linking": {
          "missing_dso_allowlist": [
            "*/libcuda.so.1"
          ]
        },
        "number": 0,
        "script": "# This is a hotfix for https://github.com/NVIDIA/cccl/issues/4967, backporting\n# https://github.com/NVIDIA/cccl/pull/4972 on the fly for the installed headers\n{%- if match(cuda_compiler_version, \"12.*\") and linux and x86_64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/x86_64-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux and aarch64 %}\nCCCL_PREPROCESSOR_H_TO_PATCH=\"$PREFIX/targets/sbsa-linux/include/cuda/std/__cccl/preprocessor.h\"\n{%- endif %}\n\n{%- if match(cuda_compiler_version, \"12.*\") and linux %}\nROOT_DIR=\"$(dirname \"${CCCL_PREPROCESSOR_H_TO_PATCH}\")/../../../..\"\npatch -p1 -d \"${ROOT_DIR}\" -i \"${RECIPE_DIR}/cuda12_cccl_arch_all.patch\"\n{%- endif %}\n\necho hello\nLLAMA_ARGS=\"-DLLAMA_BUILD_TESTS=OFF\"\n\n{%- macro llama_args(value) %}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DLLAMA_${{ value }}\"\n{%- endmacro %}\n\n{%- macro cmake_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -D${{ value }}\"\n{%- endmacro %}\n\n{% macro ggml_args(value) -%}\nLLAMA_ARGS=\"${LLAMA_ARGS} -DGGML_${{ value }}\"\n{%- endmacro %}\n\n${{ cmake_args(\"BUILD_SHARED_LIBS=ON\") }}\n${{ llama_args(\"CURL=ON\") }}\n\n{%- if osx and arm64 %}\n${{ ggml_args(\"NATIVE=OFF\") }}\n${{ ggml_args(\"AVX=OFF\") }}\n${{ ggml_args(\"AVX2=OFF\") }}\n${{ ggml_args(\"FMA=OFF\") }}\n${{ ggml_args(\"F16C=OFF\") }}\n${{ ggml_args(\"METAL=ON\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if osx and x86_64 %}\n${{ ggml_args(\"METAL=OFF\") }}\n${{ ggml_args(\"ACCELERATE=ON\") }}\n{%- endif %}\n\n{%- if cuda_compiler_version | default(\"None\") != \"None\" %}\n${{ ggml_args(\"CUDA=ON\") }}\n# Exclude sm_120 (compute capability 12.0) as it doesn't support MXFP4 instructions\n# Explicitly list all architectures except 120\n${{ cmake_args(\"CMAKE_CUDA_ARCHITECTURES=80-virtual;86-real;89-real;120a-real;121a-real\") }}\n{%- endif %}\n\n{%- if linux and x86_64 and cuda_compiler_version == \"None\" %}\n${{ ggml_args(\"BLAS=ON\") }}\n\n{%- if blas_impl == \"mkl\" %}\n${{ ggml_args(\"BLAS_VENDOR=Intel10_64_dyn\") }}\n{%- endif %}\n\n{%- endif %}\n\necho $LLAMA_ARGS\ncmake -S . -B build -G Ninja ${CMAKE_ARGS} ${LLAMA_ARGS}\ncmake --build build\ncmake --install build\n",
        "string": "cpu_mkl_h0393721_0"
      },
      "context": {
        "build": 0,
        "name": "llama.cpp",
        "version": "7700"
      },
      "extra": {
        "recipe-maintainers": [
          "traversaro",
          "jjerphan",
          "jonashaag",
          "frankier",
          "sodre",
          "pavelzw"
        ]
      },
      "package": {
        "name": "llama.cpp",
        "version": "7700"
      },
      "requirements": {
        "build": [
          "gcc_linux-64 14.*",
          "sysroot_linux-64 2.17.*",
          "gxx_linux-64 14.*",
          "cmake",
          "git",
          "ninja",
          "pkgconfig"
        ],
        "host": [
          "libcurl",
          "blas-devel * *_mkl",
          "mkl-devel 2025.*"
        ],
        "run_constraints": [
          "whisper.cpp <0.0.0a0"
        ]
      },
      "schema_version": 1,
      "source": [
        {
          "sha256": "3cfb6fbe53c6d35a4ed87870da08df2ed545c77df10e4a1a193d82cfa2b8c359",
          "url": "https://github.com/ggml-org/llama.cpp/archive/b7700.tar.gz"
        }
      ],
      "tests": [
        {
          "script": [
            "check_file() {\n  local file=\"$1\"\n  local description=\"$2\"\n  if test -f \"$file\"; then\n    echo \"\u2713 Found: $description\"\n  else\n    echo \"\u2717 Missing: $description\"\n    return 1\n  fi\n}\n",
            "echo \"Checking binary files...\"",
            "check_file \"$PREFIX/bin/convert_hf_to_gguf.py\" \"convert_hf_to_gguf.py\"",
            "check_file \"$PREFIX/bin/llama-batched\" \"llama-batched\"",
            "check_file \"$PREFIX/bin/llama-batched-bench\" \"llama-batched-bench\"",
            "check_file \"$PREFIX/bin/llama-bench\" \"llama-bench\"",
            "check_file \"$PREFIX/bin/llama-cli\" \"llama-cli\"",
            "check_file \"$PREFIX/bin/llama-convert-llama2c-to-ggml\" \"llama-convert-llama2c-to-ggml\"",
            "check_file \"$PREFIX/bin/llama-cvector-generator\" \"llama-cvector-generator\"",
            "check_file \"$PREFIX/bin/llama-embedding\" \"llama-embedding\"",
            "check_file \"$PREFIX/bin/llama-diffusion-cli\" \"llama-diffusion-cli\"",
            "check_file \"$PREFIX/bin/llama-eval-callback\" \"llama-eval-callback\"",
            "check_file \"$PREFIX/bin/llama-export-lora\" \"llama-export-lora\"",
            "check_file \"$PREFIX/bin/llama-finetune\" \"llama-finetune\"",
            "check_file \"$PREFIX/bin/llama-gen-docs\" \"llama-gen-docs\"",
            "check_file \"$PREFIX/bin/llama-gguf\" \"llama-gguf\"",
            "check_file \"$PREFIX/bin/llama-gguf-hash\" \"llama-gguf-hash\"",
            "check_file \"$PREFIX/bin/llama-gguf-split\" \"llama-gguf-split\"",
            "check_file \"$PREFIX/bin/llama-imatrix\" \"llama-imatrix\"",
            "check_file \"$PREFIX/bin/llama-lookahead\" \"llama-lookahead\"",
            "check_file \"$PREFIX/bin/llama-lookup\" \"llama-lookup\"",
            "check_file \"$PREFIX/bin/llama-lookup-create\" \"llama-lookup-create\"",
            "check_file \"$PREFIX/bin/llama-lookup-merge\" \"llama-lookup-merge\"",
            "check_file \"$PREFIX/bin/llama-lookup-stats\" \"llama-lookup-stats\"",
            "check_file \"$PREFIX/bin/llama-mtmd-cli\" \"llama-mtmd-cli\"",
            "check_file \"$PREFIX/bin/llama-parallel\" \"llama-parallel\"",
            "check_file \"$PREFIX/bin/llama-passkey\" \"llama-passkey\"",
            "check_file \"$PREFIX/bin/llama-perplexity\" \"llama-perplexity\"",
            "check_file \"$PREFIX/bin/llama-quantize\" \"llama-quantize\"",
            "check_file \"$PREFIX/bin/llama-retrieval\" \"llama-retrieval\"",
            "check_file \"$PREFIX/bin/llama-save-load-state\" \"llama-save-load-state\"",
            "check_file \"$PREFIX/bin/llama-server\" \"llama-server\"",
            "check_file \"$PREFIX/bin/llama-simple\" \"llama-simple\"",
            "check_file \"$PREFIX/bin/llama-simple-chat\" \"llama-simple-chat\"",
            "check_file \"$PREFIX/bin/llama-speculative\" \"llama-speculative\"",
            "check_file \"$PREFIX/bin/llama-speculative-simple\" \"llama-speculative-simple\"",
            "check_file \"$PREFIX/bin/llama-tokenize\" \"llama-tokenize\"",
            "check_file \"$PREFIX/bin/llama-tts\" \"llama-tts\"",
            "echo \"Checking header files...\"",
            "check_file \"$PREFIX/include/ggml-alloc.h\" \"ggml-alloc.h\"",
            "check_file \"$PREFIX/include/ggml-backend.h\" \"ggml-backend.h\"",
            "check_file \"$PREFIX/include/ggml-blas.h\" \"ggml-blas.h\"",
            "check_file \"$PREFIX/include/ggml-cann.h\" \"ggml-cann.h\"",
            "check_file \"$PREFIX/include/ggml-cpp.h\" \"ggml-cpp.h\"",
            "check_file \"$PREFIX/include/ggml-cpu.h\" \"ggml-cpu.h\"",
            "check_file \"$PREFIX/include/ggml-cuda.h\" \"ggml-cuda.h\"",
            "check_file \"$PREFIX/include/ggml-metal.h\" \"ggml-metal.h\"",
            "check_file \"$PREFIX/include/ggml-opt.h\" \"ggml-opt.h\"",
            "check_file \"$PREFIX/include/ggml-rpc.h\" \"ggml-rpc.h\"",
            "check_file \"$PREFIX/include/ggml-sycl.h\" \"ggml-sycl.h\"",
            "check_file \"$PREFIX/include/ggml-vulkan.h\" \"ggml-vulkan.h\"",
            "check_file \"$PREFIX/include/ggml-webgpu.h\" \"ggml-webgpu.h\"",
            "check_file \"$PREFIX/include/ggml.h\" \"ggml.h\"",
            "check_file \"$PREFIX/include/gguf.h\" \"gguf.h\"",
            "check_file \"$PREFIX/include/llama-cpp.h\" \"llama-cpp.h\"",
            "check_file \"$PREFIX/include/llama.h\" \"llama.h\"",
            "echo \"Checking CMake configuration files...\"",
            "check_file \"$PREFIX/lib/cmake/ggml/ggml-config.cmake\" \"ggml-config.cmake\"",
            "check_file \"$PREFIX/lib/cmake/ggml/ggml-version.cmake\" \"ggml-version.cmake\"",
            "check_file \"$PREFIX/lib/cmake/llama/llama-config.cmake\" \"llama-config.cmake\"",
            "check_file \"$PREFIX/lib/cmake/llama/llama-version.cmake\" \"llama-version.cmake\"",
            "echo \"Checking library files...\"",
            "check_file \"$PREFIX/lib/libggml-base${SHLIB_EXT}\" \"libggml-base${SHLIB_EXT}\"",
            "check_file \"$PREFIX/lib/libggml-cpu${SHLIB_EXT}\" \"libggml-cpu${SHLIB_EXT}\"",
            "check_file \"$PREFIX/lib/libggml${SHLIB_EXT}\" \"libggml${SHLIB_EXT}\"",
            "check_file \"$PREFIX/lib/libllama${SHLIB_EXT}\" \"libllama${SHLIB_EXT}\"",
            "check_file \"$PREFIX/lib/libmtmd${SHLIB_EXT}\" \"libmtmd${SHLIB_EXT}\"",
            "check_file \"$PREFIX/lib/libggml-blas${SHLIB_EXT}\" \"libggml-blas${SHLIB_EXT}\"",
            "echo \"Checking pkg-config file...\"",
            "check_file \"$PREFIX/lib/pkgconfig/llama.pc\" \"llama.pc\""
          ]
        },
        {
          "script": [
            "llama-cli --help",
            "llama-server --help"
          ]
        }
      ]
    },
    "system_tools": {
      "rattler-build": "0.55.1"
    }
  }
]
