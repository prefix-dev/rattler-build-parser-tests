context:
  version: 0.10.2
  build_number: 0
  use_cuda: ${{ cuda_compiler_version != "None" }}
  pytorch_version: 2.8.0
  vllm_target_device: ${{ "cuda" if use_cuda else "cpu" }}
  cuda_build_string: cuda_${{ cuda_compiler_version | version_to_buildstring }}
  string_prefix: ${{ cuda_build_string if cuda_compiler_version != "None" else "cpu_" }}
  is_cross_compiling: ${{ build_platform != host_platform }}

package:
  name: vllm
  version: ${{ version }}

source:
  - url: https://pypi.org/packages/source/v/vllm/vllm-${{ version }}.tar.gz
    sha256: 57608f44cf61f5d80fb182c98e06e524cb2925bb528258a7b247c8e43a52d13e
    patches:
      - patches/0001-Search-for-the-CUDA-package-in-CMakeLists.patch
      - patches/0002-Remove-ninja-pip-requirement.patch
      - if: linux
        then:
          - patches/0003-Manually-define-gettid.patch
      - if: is_cross_compiling
        then:
          - patches/0004-Factor-in-the-cmake-args-when-building-e.g.-for-cros.patch
      - if: aarch64 or arm64
        then:
          - patches/0005-Configure-build-to-target-aarch64-even-though-CMake-.patch
      - patches/0006-Use-consistent-pytorch-version-across-builds.patch
      - patches/0007-pass-required-CUTLASS_NVCC_ARCHS-through-to-cutlass.patch
  # Needs to be vendored because vLLM uses a modified version of the flash attention primitives that supports KV-caching.
  # see https://github.com/vllm-project/vllm/blob/v{{ version }}/cmake/external_projects/vllm_flash_attn.cmake#L41
  - url: https://github.com/vllm-project/flash-attention/archive/ee4d25bd84e0cbc7e0b9b9685085fd5db2dcb62a.tar.gz
    sha256: 43336897cb2f264bf607778705b6986f2559bcc36a9df5c78d340ca5b3885183
    target_directory: flash-attention

build:
  number: ${{ build_number }}
  string: ${{ string_prefix }}py${{ python | version_to_buildstring }}h${{ hash }}_${{ build_number }}
  script:
    - sed -i.bak 's/set(TORCH_SUPPORTED_VERSION_CUDA "2.4.0")/set(TORCH_SUPPORTED_VERSION_CUDA "${{ pytorch_version }}")/g' flash-attention/CMakeLists.txt
    - export VLLM_FLASH_ATTN_SRC_DIR=$SRC_DIR/flash-attention
    - python use_existing_torch.py
    - mkdir -p $SRC_DIR/third_party/NVTX/c
    - ln -s $PREFIX/include $SRC_DIR/third_party/NVTX/c/include
    - export VERBOSE=1
    - export VLLM_TARGET_DEVICE=${{ vllm_target_device }}
    - if: use_cuda
      then:
        - export TORCH_NVCC_FLAGS="-Xfatbin -compress-all"
        # Building vLLM is memory-intensive: see https://github.com/Dao-AILab/flash-attention/issues/1043#issuecomment-2770635000
        - export MAX_JOBS=3
        # Override the CUDA architectures configured in the conda-forge nvcc package: https://github.com/conda-forge/cuda-nvcc-feedstock/blob/7843e9f1b9ea6bc555cd70c247d774189fc34110/recipe/conda_build_config.yaml#L21-L28
        - export CUDAARCHS="70-real;80-real;86-real;89-real;90a-real;100f-real;120a-real"
        - export TORCH_CUDA_ARCH_LIST="7.0;8.0;8.6;8.9;9.0;10.0;12.0+PTX"
        # not usable with pytorch: https://github.com/pytorch/pytorch/issues/172034
        - export CMAKE_CUDA_ARCHITECTURES="70;80;86;89;90;100;120"
        # for vendored cutlass, see https://github.com/NVIDIA/cutlass/blob/v4.0.0/CMakeLists.txt
        - export CUTLASS_NVCC_ARCHS="70;80;86;89;90;100;120"
    # CMake is unable to automatically locate the Python include dir for aarch64 for some reason
    - if: aarch64
      then:
        - export CMAKE_ARGS="$CMAKE_ARGS -DPython_INCLUDE_DIR="$(python -c 'import sysconfig; print(sysconfig.get_path("include"))')""
    # ensure we don't use the default CMAKE_BUILD_TYPE=RelWithDebInfo, see
    # https://github.com/vllm-project/vllm/blob/v0.10.1/setup.py#L147
    # https://github.com/vllm-project/vllm/blob/v0.10.1/vllm/envs.py#L268
    - export CMAKE_BUILD_TYPE=Release
    - ${{ PYTHON }} -m pip install . -vv --no-build-isolation --no-deps

  python:
    entry_points:
      - vllm = vllm.entrypoints.cli.main:main

  skip:
    - win
    - osx and x86_64
    # ray (CUDA-only dependency) does not support 3.10 anymore
    - match(python, "<3.11") and cuda_compiler_version != "None"
    - aarch64 and use_cuda  # Still have issues locating CUDA for the aarch64 build
    # - not use_cuda  # Just build CUDA for now
    # - match(python, "!=3.10")  # Until all the builds succeed

requirements:
  build:
    - cmake >=3.26.1
    - git
    - ninja
    - zlib
    - ${{ stdlib('c') }}
    - ${{ compiler('c') }}
    - ${{ compiler('cxx') }}
    - if: use_cuda
      then:
        - ${{ compiler('cuda') }}
    - if: is_cross_compiling
      then:
        - python
        - cross-python_${{ host_platform }}
        - pytorch ==${{ pytorch_version }}
        - if: use_cuda
          then:
            - pytorch * [build=cuda*]
  host:
    - python
    - jinja2 >=3.1.6
    - packaging >=24.2
    - pip
    - pytorch ==${{ pytorch_version }}
    - regex
    - setuptools >=77.0.3,<80.0.0
    - setuptools-scm >=8
    - wheel
    - if: linux
      then:
        - libnuma
    - if: use_cuda
      then:
        - pytorch * [build=cuda*]
        - cuda
        - cuda-cudart-dev
        - cuda-nvrtc-dev
        - cuda-nvrtc-static
        - cuda-version ==${{ cuda_compiler_version }}
        - cutlass <4  # Cutlass 4 introduces some major changes to the API that causes it to not compile
        - libcublas-dev
        - nvtx-c
  run:
    - python
    # see https://github.com/vllm-project/vllm/blame/v{{ version }}/requirements/common.txt
    # can save raw content into multiline variable `reqs` and then preprocess with
    # print("\n    - ".join(sorted([x.split("#")[0] for x in reqs.splitlines()])))
    # if done like that, it obviously needs manual fix-ups though (check the diff!)
    - aiohttp
    - blake3
    - cachetools
    - cbor2
    - cloudpickle
    - compressed-tensors ==0.11.0
    - depyf ==0.19.0
    - diskcache ==5.6.3
    - einops
    - fastapi >=0.115.0
    - filelock >=3.16.1
    - gguf >=0.13.0
    - lark ==1.2.2
    - llguidance >=0.7.11,<0.8.0
    - lm-format-enforcer ==0.11.3
    - mistral-common >=1.8.2
    - msgspec
    - ninja
    - numba ==0.61.2
    - numpy
    - openai >=1.99.1
    - openai-harmony >=0.0.3
    - opencv >=4.11.0
    - outlines-core ==0.2.11
    - partial-json-parser
    - pillow
    - prometheus-fastapi-instrumentator >=7.0.0
    - prometheus_client >=0.18.0
    - protobuf
    - psutil
    - py-cpuinfo
    - pybase64
    - pydantic >=2.11.7
    - python-json-logger
    - pytorch ==${{ pytorch_version }}
    - pyyaml
    - pyzmq >=25.0.0
    - regex
    - requests >=2.26.0
    - scipy
    - sentencepiece
    - setproctitle
    - tiktoken >=0.6.0
    - tokenizers >=0.21.1
    - tqdm
    - transformers >=4.55.2
    - typing_extensions >=4.10
    - watchfiles
    - xgrammar ==0.1.23
    - if: match(python, ">3.11")
      then:
        - six >=1.16.0
        - setuptools >=77.0.3,<80
    - if: use_cuda
      then:
        # see # see https://github.com/vllm-project/vllm/blame/v{{ version }}/requirements/cuda.txt
        - pytorch ==${{ pytorch_version }} [build=cuda*]
        - ray-cgraph >=2.48
        - torchaudio ==${{ pytorch_version }}
        - torchvision ==0.23.0
        - if: linux64
          then:
            - xformers ==0.0.32.*
      else:
        # see https://github.com/vllm-project/vllm/blame/v{{ version }}/requirements/cpu.txt;
        # some are in common already (py-cpuinfo), some move there because they're in both cpu&cuda (numba)
        - packaging >=24.2
        - torchaudio
        - torchvision
        - if: x86_64
          then:
            - triton ==3.2.0
  run_constraints:
    # Fixes issue with incompatibility between old `datasets` versions and `pyarrow` v21+
    # See https://github.com/apache/arrow/issues/47155 for more details.
    # The required PR is: https://github.com/huggingface/datasets/pull/6404
    - datasets >=2.15
  ignore_run_exports:
    from_package:
      - cuda-nvrtc-dev
      - libcublas-dev
tests:
  - python:
      imports:
        - vllm
        - if: linux and use_cuda
          then:
            - vllm.vllm_flash_attn
      pip_check: false
  - script:
    # As of vllm v0.9 and later, it seems like libcuda.so.1 is required for the CLI for CUDA builds (stub libraries don't work)
    # We can't test this on the CPU runners, which is what we're using to build the wheel
    - if: not use_cuda
      then:
        - vllm --version
  - files:
      source:
        - tests/
    requirements:
      run:
        # see https://github.com/vllm-project/vllm/blame/v{{ version }}/requirements/test.in
        # currently just a small subset
        - huggingface_hub
        - pytest
        - tblib
    script:
      # Pick an arbitrary test to run: some of the other ones rely on a bunch of external packages
      - pytest ./tests/core/test_scheduler.py

about:
  homepage: https://github.com/vllm-project/vllm
  summary: A high-throughput and memory-efficient inference and serving engine for LLMs
  description: Easy, fast, and cheap LLM serving for everyone
  license: Apache-2.0 AND BSD-3-Clause
  license_file:
    - LICENSE
    - flash-attention/LICENSE
    - LICENSE_CUTLASS.txt
  documentation: https://vllm.readthedocs.io/en/latest/

extra:
  recipe-maintainers:
    - maresb
    - shermansiu
    - h-vetinari
